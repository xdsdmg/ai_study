\chapter{线性回归}

为了让我们的住房案例更有趣，让我们考虑一个稍微复杂些的数据集，我们额外知晓每套住房的卧室数量：

\begin{center}
  \begin{tabular}{c|c|c}
    居住面积（平方英尺） & \#卧室数量 & 价格（1000美元） \\
    \hline
    2104                 & 3          & 400              \\
    1600                 & 3          & 330              \\
    2400                 & 3          & 369              \\
    1416                 & 2          & 232              \\
    3000                 & 4          & 540              \\
    $\vdots$             & $\vdots$   & $\vdots$         \\
  \end{tabular}
\end{center}

其中，$x$为属于$\mathbb{R}^2$的二维向量。例如，$x_1^{(i)}$为训练集中第$i$套住房的居住面积，$x_2^{(i)}$为其卧室数量。（通常，当设计一个学习问题时，需要由你自己来决定选择哪些特征，因此，如果你在波特兰（Portland）收集住房数据，可能也会选择其他特征，比如，每套住房是否有壁炉及浴室的数量等。我们后续会讨论更多有关于特征选择的内容，但目前只考虑上面给出的特征。）

为了开展监督学习，我们必须决定如何在计算机中表示函数或假设$h$。作为初始选择，我们将$y$近似为一个关于$x$的线性函数：
$$
  h_\theta(x)=\theta_0+\theta_1 x_1 + \theta_2 x_2
$$
其中，$\theta_i$为参数（也称为权重），参数化从$\mathscr{X}$映射到$\mathcal{Y}$的线性函数空间。我们将$h_\theta(x)$简写为$h(x)$。为了简化我们的表示，我们引入$x_0=1$（截距项，Intercept Term），可以得到如下等式：
$$
h(x)=\sum^d_{i=0}\theta_i x_i = \theta^T x
$$
其中，我们可以将上述等式右侧的$\theta$与$x$视为向量，$d$为输入变量的数量（不计算$x_0$）。

% LMS 算法
\input{supervised_learning/linear_regression/lms_algorithm}

